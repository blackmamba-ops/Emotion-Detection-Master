{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/blackmamba-ops/Tomato_vision/blob/main/CNN_Project.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/blackmamba-ops/Emotion-Detection-Master.git\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"  # Change this to \"display\" if you want to display emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define data generators\n",
    "train_dir = r'C:\\Users\\BASIL K AJI\\Desktop\\Projects\\Emotion-detection-master\\data\\train'\n",
    "val_dir = r'C:\\Users\\BASIL K AJI\\Desktop\\Projects\\Emotion-detection-master\\data\\test'\n",
    "\n",
    "num_train = 28709\n",
    "num_val = 7178\n",
    "batch_size = 64\n",
    "num_epoch = 100\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BASIL K AJI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and display emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BASIL K AJI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 806ms/step - accuracy: 0.2485 - loss: 1.8287 - val_accuracy: 0.3516 - val_loss: 1.6738\n",
      "Epoch 2/100\n",
      "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 261ms/step - accuracy: 0.2188 - loss: 1.6926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BASIL K AJI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2188 - loss: 0.8482 - val_accuracy: 0.4000 - val_loss: 0.8535\n",
      "Epoch 3/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 322ms/step - accuracy: 0.3569 - loss: 1.6434 - val_accuracy: 0.4141 - val_loss: 1.5348\n",
      "Epoch 4/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - accuracy: 0.3750 - loss: 0.7818 - val_accuracy: 0.3000 - val_loss: 0.7578\n",
      "Epoch 5/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 272ms/step - accuracy: 0.4040 - loss: 1.5466 - val_accuracy: 0.4410 - val_loss: 1.4656\n",
      "Epoch 6/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4844 - loss: 0.7246 - val_accuracy: 0.4000 - val_loss: 0.6780\n",
      "Epoch 7/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 278ms/step - accuracy: 0.4378 - loss: 1.4688 - val_accuracy: 0.4717 - val_loss: 1.3982\n",
      "Epoch 8/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4219 - loss: 0.7606 - val_accuracy: 0.5000 - val_loss: 0.7490\n",
      "Epoch 9/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 310ms/step - accuracy: 0.4627 - loss: 1.4043 - val_accuracy: 0.4770 - val_loss: 1.3621\n",
      "Epoch 10/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.4688 - loss: 0.7445 - val_accuracy: 0.4000 - val_loss: 0.7547\n",
      "Epoch 11/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 334ms/step - accuracy: 0.4859 - loss: 1.3515 - val_accuracy: 0.5029 - val_loss: 1.3084\n",
      "Epoch 12/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4688 - loss: 0.7019 - val_accuracy: 0.4000 - val_loss: 0.7320\n",
      "Epoch 13/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 308ms/step - accuracy: 0.5092 - loss: 1.2995 - val_accuracy: 0.5153 - val_loss: 1.2789\n",
      "Epoch 14/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4688 - loss: 0.7719 - val_accuracy: 0.4000 - val_loss: 0.7137\n",
      "Epoch 15/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 328ms/step - accuracy: 0.5244 - loss: 1.2631 - val_accuracy: 0.5345 - val_loss: 1.2378\n",
      "Epoch 16/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4844 - loss: 0.5946 - val_accuracy: 0.7000 - val_loss: 0.4786\n",
      "Epoch 17/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 307ms/step - accuracy: 0.5353 - loss: 1.2318 - val_accuracy: 0.5392 - val_loss: 1.2184\n",
      "Epoch 18/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5938 - loss: 0.5874 - val_accuracy: 0.7000 - val_loss: 0.5287\n",
      "Epoch 19/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 297ms/step - accuracy: 0.5407 - loss: 1.2124 - val_accuracy: 0.5459 - val_loss: 1.1963\n",
      "Epoch 20/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914us/step - accuracy: 0.5938 - loss: 0.5433 - val_accuracy: 0.7000 - val_loss: 0.5385\n",
      "Epoch 21/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 277ms/step - accuracy: 0.5607 - loss: 1.1708 - val_accuracy: 0.5495 - val_loss: 1.1891\n",
      "Epoch 22/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - accuracy: 0.5469 - loss: 0.6172 - val_accuracy: 0.6000 - val_loss: 0.6209\n",
      "Epoch 23/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.5646 - loss: 1.1480 - val_accuracy: 0.5590 - val_loss: 1.1668\n",
      "Epoch 24/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5781 - loss: 0.5330 - val_accuracy: 0.4000 - val_loss: 0.7362\n",
      "Epoch 25/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 303ms/step - accuracy: 0.5878 - loss: 1.1123 - val_accuracy: 0.5674 - val_loss: 1.1470\n",
      "Epoch 26/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5625 - loss: 0.5309 - val_accuracy: 0.5000 - val_loss: 0.4883\n",
      "Epoch 27/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 282ms/step - accuracy: 0.5982 - loss: 1.0855 - val_accuracy: 0.5732 - val_loss: 1.1306\n",
      "Epoch 28/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5312 - loss: 0.5870 - val_accuracy: 0.5000 - val_loss: 0.6897\n",
      "Epoch 29/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.6085 - loss: 1.0618 - val_accuracy: 0.5713 - val_loss: 1.1317\n",
      "Epoch 30/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - accuracy: 0.5312 - loss: 0.5344 - val_accuracy: 0.3000 - val_loss: 0.7551\n",
      "Epoch 31/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 283ms/step - accuracy: 0.6201 - loss: 1.0288 - val_accuracy: 0.5801 - val_loss: 1.1039\n",
      "Epoch 32/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6094 - loss: 0.5023 - val_accuracy: 0.6000 - val_loss: 0.6448\n",
      "Epoch 33/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 312ms/step - accuracy: 0.6203 - loss: 1.0192 - val_accuracy: 0.5808 - val_loss: 1.1140\n",
      "Epoch 34/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5781 - loss: 0.6282 - val_accuracy: 0.4000 - val_loss: 0.7845\n",
      "Epoch 35/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 320ms/step - accuracy: 0.6285 - loss: 0.9953 - val_accuracy: 0.5866 - val_loss: 1.1001\n",
      "Epoch 36/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6250 - loss: 0.5841 - val_accuracy: 0.5000 - val_loss: 0.7156\n",
      "Epoch 37/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 326ms/step - accuracy: 0.6454 - loss: 0.9634 - val_accuracy: 0.5935 - val_loss: 1.0837\n",
      "Epoch 38/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6250 - loss: 0.5240 - val_accuracy: 0.3000 - val_loss: 0.9481\n",
      "Epoch 39/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.6527 - loss: 0.9419 - val_accuracy: 0.5953 - val_loss: 1.0835\n",
      "Epoch 40/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6406 - loss: 0.4787 - val_accuracy: 0.7000 - val_loss: 0.3683\n",
      "Epoch 41/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.6639 - loss: 0.9121 - val_accuracy: 0.6009 - val_loss: 1.0788\n",
      "Epoch 42/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6719 - loss: 0.4664 - val_accuracy: 0.9000 - val_loss: 0.2745\n",
      "Epoch 43/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 324ms/step - accuracy: 0.6696 - loss: 0.8985 - val_accuracy: 0.6006 - val_loss: 1.0782\n",
      "Epoch 44/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6719 - loss: 0.4786 - val_accuracy: 0.8000 - val_loss: 0.2308\n",
      "Epoch 45/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 322ms/step - accuracy: 0.6793 - loss: 0.8718 - val_accuracy: 0.5967 - val_loss: 1.0814\n",
      "Epoch 46/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6562 - loss: 0.4562 - val_accuracy: 0.5000 - val_loss: 0.6917\n",
      "Epoch 47/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 324ms/step - accuracy: 0.6859 - loss: 0.8497 - val_accuracy: 0.6018 - val_loss: 1.0630\n",
      "Epoch 48/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6094 - loss: 0.4193 - val_accuracy: 0.7000 - val_loss: 0.6221\n",
      "Epoch 49/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 328ms/step - accuracy: 0.7003 - loss: 0.8201 - val_accuracy: 0.6060 - val_loss: 1.0624\n",
      "Epoch 50/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7031 - loss: 0.4128 - val_accuracy: 0.3000 - val_loss: 0.7322\n",
      "Epoch 51/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 330ms/step - accuracy: 0.7129 - loss: 0.7956 - val_accuracy: 0.6024 - val_loss: 1.0833\n",
      "Epoch 52/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7344 - loss: 0.4385 - val_accuracy: 0.9000 - val_loss: 0.1670\n",
      "Epoch 53/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 328ms/step - accuracy: 0.7143 - loss: 0.7788 - val_accuracy: 0.6123 - val_loss: 1.0612\n",
      "Epoch 54/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7344 - loss: 0.3521 - val_accuracy: 0.7000 - val_loss: 0.3293\n",
      "Epoch 55/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 325ms/step - accuracy: 0.7247 - loss: 0.7460 - val_accuracy: 0.6117 - val_loss: 1.0696\n",
      "Epoch 56/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7031 - loss: 0.4794 - val_accuracy: 0.6000 - val_loss: 0.7070\n",
      "Epoch 57/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 327ms/step - accuracy: 0.7340 - loss: 0.7308 - val_accuracy: 0.6126 - val_loss: 1.0685\n",
      "Epoch 58/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6719 - loss: 0.4042 - val_accuracy: 0.6000 - val_loss: 0.4117\n",
      "Epoch 59/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 326ms/step - accuracy: 0.7446 - loss: 0.7021 - val_accuracy: 0.6165 - val_loss: 1.0699\n",
      "Epoch 60/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6719 - loss: 0.3577 - val_accuracy: 0.6000 - val_loss: 0.4966\n",
      "Epoch 61/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 329ms/step - accuracy: 0.7549 - loss: 0.6735 - val_accuracy: 0.6166 - val_loss: 1.0688\n",
      "Epoch 62/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.3404 - val_accuracy: 0.5000 - val_loss: 0.6059\n",
      "Epoch 63/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.7593 - loss: 0.6595 - val_accuracy: 0.6136 - val_loss: 1.0806\n",
      "Epoch 64/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6094 - loss: 0.4034 - val_accuracy: 0.5000 - val_loss: 0.6953\n",
      "Epoch 65/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.7688 - loss: 0.6330 - val_accuracy: 0.6113 - val_loss: 1.0887\n",
      "Epoch 66/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7656 - loss: 0.3238 - val_accuracy: 0.6000 - val_loss: 0.4334\n",
      "Epoch 67/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 321ms/step - accuracy: 0.7693 - loss: 0.6212 - val_accuracy: 0.6152 - val_loss: 1.1019\n",
      "Epoch 68/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7969 - loss: 0.2914 - val_accuracy: 0.6000 - val_loss: 0.4385\n",
      "Epoch 69/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 326ms/step - accuracy: 0.7885 - loss: 0.5854 - val_accuracy: 0.6189 - val_loss: 1.0950\n",
      "Epoch 70/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6875 - loss: 0.3849 - val_accuracy: 0.8000 - val_loss: 0.4085\n",
      "Epoch 71/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 322ms/step - accuracy: 0.7935 - loss: 0.5755 - val_accuracy: 0.6232 - val_loss: 1.1066\n",
      "Epoch 72/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.2665 - val_accuracy: 0.6000 - val_loss: 0.5263\n",
      "Epoch 73/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.7961 - loss: 0.5627 - val_accuracy: 0.6211 - val_loss: 1.1094\n",
      "Epoch 74/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.2690 - val_accuracy: 0.7000 - val_loss: 0.2561\n",
      "Epoch 75/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - accuracy: 0.8020 - loss: 0.5445 - val_accuracy: 0.6240 - val_loss: 1.1029\n",
      "Epoch 76/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7188 - loss: 0.3501 - val_accuracy: 0.6000 - val_loss: 0.4420\n",
      "Epoch 77/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 325ms/step - accuracy: 0.8157 - loss: 0.5141 - val_accuracy: 0.6191 - val_loss: 1.1056\n",
      "Epoch 78/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7344 - loss: 0.3146 - val_accuracy: 0.7000 - val_loss: 0.4623\n",
      "Epoch 79/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 308ms/step - accuracy: 0.8178 - loss: 0.5059 - val_accuracy: 0.6264 - val_loss: 1.1135\n",
      "Epoch 80/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.3096 - val_accuracy: 0.5000 - val_loss: 0.7388\n",
      "Epoch 81/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 280ms/step - accuracy: 0.8232 - loss: 0.4835 - val_accuracy: 0.6243 - val_loss: 1.1166\n",
      "Epoch 82/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7344 - loss: 0.3122 - val_accuracy: 0.7000 - val_loss: 0.5394\n",
      "Epoch 83/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.8314 - loss: 0.4666 - val_accuracy: 0.6267 - val_loss: 1.1333\n",
      "Epoch 84/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7656 - loss: 0.3394 - val_accuracy: 0.5000 - val_loss: 0.4770\n",
      "Epoch 85/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.8371 - loss: 0.4502 - val_accuracy: 0.6177 - val_loss: 1.1540\n",
      "Epoch 86/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.1598 - val_accuracy: 0.7000 - val_loss: 0.4064\n",
      "Epoch 87/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.8466 - loss: 0.4310 - val_accuracy: 0.6267 - val_loss: 1.1560\n",
      "Epoch 88/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7969 - loss: 0.2627 - val_accuracy: 0.6000 - val_loss: 0.6365\n",
      "Epoch 89/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 279ms/step - accuracy: 0.8480 - loss: 0.4222 - val_accuracy: 0.6249 - val_loss: 1.1663\n",
      "Epoch 90/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.0963 - val_accuracy: 0.7000 - val_loss: 0.7235\n",
      "Epoch 91/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 278ms/step - accuracy: 0.8545 - loss: 0.4081 - val_accuracy: 0.6240 - val_loss: 1.1773\n",
      "Epoch 92/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8125 - loss: 0.2115 - val_accuracy: 0.9000 - val_loss: 0.1457\n",
      "Epoch 93/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 278ms/step - accuracy: 0.8572 - loss: 0.3973 - val_accuracy: 0.6237 - val_loss: 1.1743\n",
      "Epoch 94/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.2377 - val_accuracy: 0.7000 - val_loss: 0.5694\n",
      "Epoch 95/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 278ms/step - accuracy: 0.8614 - loss: 0.3889 - val_accuracy: 0.6223 - val_loss: 1.1849\n",
      "Epoch 96/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8281 - loss: 0.1814 - val_accuracy: 0.5000 - val_loss: 0.6973\n",
      "Epoch 97/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 276ms/step - accuracy: 0.8699 - loss: 0.3707 - val_accuracy: 0.6221 - val_loss: 1.1967\n",
      "Epoch 98/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1303 - val_accuracy: 0.6000 - val_loss: 0.6224\n",
      "Epoch 99/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 282ms/step - accuracy: 0.8671 - loss: 0.3637 - val_accuracy: 0.6204 - val_loss: 1.2076\n",
      "Epoch 100/100\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9062 - loss: 0.1819 - val_accuracy: 0.7000 - val_loss: 0.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "    model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            epochs=num_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val // batch_size,\n",
    "            callbacks=[tensorboard_callback])\n",
    "    \n",
    "    model.save('model.h5')\n",
    "\n",
    "# Emotions will be displayed on your face from the webcam feed\n",
    "elif mode == \"display\":\n",
    "    model = load_model('model.h5')\n",
    "\n",
    "    # Initialize the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Load the Haarcascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Define the emotion labels\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # Start capturing frames\n",
    "    with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert the frame to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the grayscale frame\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "            # Process each detected face\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Detect landmarks in the face\n",
    "                results = face_mesh.process(frame)\n",
    "\n",
    "                # Draw landmarks on the frame\n",
    "                if results.multi_face_landmarks:\n",
    "                    for landmarks in results.multi_face_landmarks:\n",
    "                        for landmark in landmarks.landmark:\n",
    "                            cx, cy = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n",
    "                            cv2.circle(frame, (cx, cy), 2, (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "                # Crop the face region\n",
    "                face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "                # Resize the face region to 48x48 pixels (required input size for the model)\n",
    "                face_roi_resized = cv2.resize(face_roi, (48, 48))\n",
    "\n",
    "                # Preprocess the face region for prediction\n",
    "                face_roi_resized = np.expand_dims(np.expand_dims(face_roi_resized, -1), 0) / 255.0\n",
    "\n",
    "                # Predict emotion\n",
    "                prediction = model.predict(face_roi_resized)\n",
    "                max_index = int(np.argmax(prediction))\n",
    "                emotion = emotion_dict[max_index]\n",
    "\n",
    "                # Draw text showing the predicted emotion on the frame\n",
    "                cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display the frame with predicted emotion and landmarks\n",
    "            cv2.imshow('Emotion Detection with MediaPipe Landmarks', frame)\n",
    "\n",
    "            # Check for 'q' key press to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Release the webcam and close any open windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
